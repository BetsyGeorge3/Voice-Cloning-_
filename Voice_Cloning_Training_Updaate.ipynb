{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Voice Cloning Training Updaate.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h5duLjpRImk8"
      },
      "source": [
        "# Voice Cloning App - Remote Training\n",
        "\n",
        "Remote training for the Voice Cloning App.\n",
        "\n",
        "**Please ensure you have this notebook enabled with GPU before running (Runtime->Change runtime type)**\n",
        "\n",
        "Steps:\n",
        "1. Export your dataset from the app & unzip\n",
        "2. Create a folder called `Voice-Cloning` in your Google Drive\n",
        "  1. Create a sub-folder called `datasets` and upload your dataset folder to it\n",
        "  2. If using non-English: Create a sub folder called `alphabets` and upload your alphabet.txt file to it \n",
        "3. Configure parameters below\n",
        "4. Run this notebook one cell at a time\n",
        "  - Connect to your google drive when prompted\n",
        "  - Ensure you have selected the correct options before running the training cell\n",
        "\n",
        "During training and once training is done you'll find your latest checkpoint in Google Drive within the folder `Voice-Cloning/checkpoints/dataset_name`.\n",
        "\n",
        "This can be download from your drive and imported into the app under the \"Import/Export\" menu."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rO7uu0M3IF4l",
        "cellView": "form"
      },
      "source": [
        "#@title Connect to google drive\n",
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "output_directory = \"/content/drive/MyDrive/Voice-Cloning\"\n",
        "os.makedirs(output_directory, exist_ok=True)\n",
        "\n",
        "dataset_directory = os.path.join(output_directory, \"datasets\")\n",
        "os.makedirs(output_directory, exist_ok=True)\n",
        "\n",
        "checkpoint_directory = os.path.join(output_directory, \"checkpoints\")\n",
        "os.makedirs(checkpoint_directory, exist_ok=True)\n",
        "\n",
        "alphabet_directory = os.path.join(output_directory, \"alphabets\")\n",
        "os.makedirs(alphabet_directory, exist_ok=True)\n",
        "\n",
        "datasets = os.listdir(dataset_directory)\n",
        "assert datasets, \"No datasets found in 'Voice-Cloning/datasets'. Please export your dataset from the app, unzip and upload to this folder\"\n",
        "\n",
        "# Check datasets\n",
        "for dataset in datasets:\n",
        "  try:\n",
        "    dataset_path = os.path.join(dataset_directory, dataset)\n",
        "    files = os.listdir(dataset_path)\n",
        "    assert \"metadata.csv\" in files, f\"Dataset '{dataset}' is missing metadata.csv\"\n",
        "    assert \"wavs\" in files, f\"Dataset '{dataset}' is missing wavs folder\"\n",
        "  except NotADirectoryError:\n",
        "    raise Exception(f\"Dataset '{dataset}' is not a folder. Please ensure all datasets are folders containing your metadata.csv & wavs\")\n",
        "\n",
        "checkpoints = {dataset: os.listdir(os.path.join(checkpoint_directory, dataset)) for dataset in datasets if os.path.isdir(os.path.join(checkpoint_directory, dataset))}\n",
        "languages = os.listdir(alphabet_directory)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JxSWGygYnz6R"
      },
      "source": [
        "#@title Parameters\n",
        "import torch\n",
        "assert torch.cuda.is_available(),  \"Please change Runtime type to GPU (Runtime->Change runtime type)\"\n",
        "\n",
        "# Clone the app\n",
        "!pip install pysrt==1.1.2 pydub==0.24.1 webrtcvad==2.0.10 Unidecode==1.0.22 gdown\n",
        "!git clone https://github.com/BenAAndrew/Voice-Cloning-App.git\n",
        "%cd /content/Voice-Cloning-App/\n",
        "!git checkout 0ad83edb6ca8251c1c3426989af0e31641fe8ec2\n",
        "from training.train import train\n",
        "from training import DEFAULT_ALPHABET\n",
        "from training.utils import load_symbols\n",
        "\n",
        "# Download pretrained model\n",
        "import gdown\n",
        "transfer_learning_path = \"/content/drive/MyDrive/Voice-Cloning/pretrained.pt\"\n",
        "gdown.download('1c5ZTuT7J08wLUoVZ2KkUs_VdZuJ86ZqA', \"./\"+transfer_learning_path, quiet=False)\n",
        "\n",
        "# Get settings\n",
        "epochs = 1000 #@param {type:\"slider\", min:100, max:3500, step:100}\n",
        "batch_size = 38 #@param {type:\"slider\", min:12, max:70, step:2}\n",
        "checkpoint_frequency = 1000 #@param {type:\"slider\", min:250, max:2500, step:250}\n",
        "backup_checkpoint_frequency = 10000 #@param {type:\"slider\", min:2500, max:25000, step:500}\n",
        "validation_size = 0.2 #@param {type:\"slider\", min:0.05, max:0.2, step:0.025}\n",
        "early_stopping = True #@param {type:\"boolean\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FFOibwS4H6UC"
      },
      "source": [
        "#@title Options\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display\n",
        "\n",
        "# Form\n",
        "dataset = widgets.Dropdown(\n",
        "    options=datasets,\n",
        "    description='Dataset:',\n",
        ")\n",
        "\n",
        "def on_change(change):\n",
        "    if change['type'] == 'change' and change['name'] == 'value':\n",
        "      checkpoint.options = checkpoints.get(change['new'], [])\n",
        "\n",
        "dataset.observe(on_change)\n",
        "\n",
        "checkpoint = widgets.Dropdown(\n",
        "    options=checkpoints.get(dataset.value, []),\n",
        "    description='Checkpoint:',\n",
        ")\n",
        "\n",
        "alphabet = widgets.Dropdown(\n",
        "    options=languages,\n",
        "    description='Language:',\n",
        ")\n",
        "\n",
        "button = widgets.Button(\n",
        "    description=\"Start training\", \n",
        "    button_style=\"success\",\n",
        ")\n",
        "\n",
        "display(dataset)\n",
        "display(checkpoint)\n",
        "if languages:\n",
        "  display(alphabet)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Train\n",
        "\n",
        "dataset_path = os.path.join(dataset_directory, dataset.value)\n",
        "metadata = os.path.join(dataset_path, \"metadata.csv\")\n",
        "wavs = os.path.join(dataset_path, \"wavs\")\n",
        "output_directory = os.path.join(checkpoint_directory, dataset.value)\n",
        "symbols = load_symbols(os.path.join(alphabet_directory, alphabet.value)) if alphabet.value else DEFAULT_ALPHABET\n",
        "checkpoint_path = os.path.join(checkpoint_directory, dataset.value, checkpoint.value) if checkpoint.value else None\n",
        "train(\n",
        "    metadata_path=metadata,\n",
        "    audio_directory=wavs,\n",
        "    output_directory=output_directory,\n",
        "    symbols=symbols,\n",
        "    checkpoint_path=checkpoint_path,\n",
        "    transfer_learning_path=transfer_learning_path,\n",
        "    epochs=epochs,\n",
        "    batch_size=batch_size,\n",
        "    early_stopping=early_stopping,\n",
        "    multi_gpu=False,\n",
        "    iters_per_checkpoint=checkpoint_frequency,\n",
        "    iters_per_backup_checkpoint=backup_checkpoint_frequency,\n",
        "    train_size=1-validation_size,\n",
        ")"
      ],
      "metadata": {
        "id": "DgTP1r7_L7Mv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}